{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artificial Neural Networks\n",
    "$\\space\\space\\space\\space\\space $ A simple Artifical Neural Netwwork (ANN) is built (see Fig 1). \n",
    "\n",
    "$\\space\\space\\space\\space\\space\\space v$ is defined by:$\\space v = x_1w_1 + x_2w_2 +...+ x_nw_n + b,$ , where $w = weight$, and $b = bias,$\n",
    "\n",
    "$\\space\\space\\space\\space\\space\\space \\sigma(v)$, a transfer function, is known as the sigmoid function: $\\space \\sigma(v) = \\frac{1}{1+e^{-v}},$ \n",
    "\n",
    "$\\space\\space\\space\\space\\space $ and y is the output, defined by: $\\space y=\\sigma(v).$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exampe 1 (Fig.1)\n",
    "import numpy as np\n",
    "w1, w2 = 20, 20\n",
    "b = -30\n",
    "def sigmoid(v):\n",
    "    return 1/(1+np.exp(-v))\n",
    "def AND(x1, x2):\n",
    "    return sigmoid(x1*w1 + x2*w2 + b)\n",
    "print(\"AND(0,0)=\", AND(0,0))\n",
    "print(\"AND(1,0)=\", AND(1,0))\n",
    "print(\"AND(0,1)=\", AND(0,1))\n",
    "print(\"AND(1,1)=\", AND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fig.1 Artificial Neural Networks`\n",
    "\n",
    "![Fig1](ai_fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward and Backpropagation Algorithm\n",
    "Assume a mean squared error, and update the weights in order to minimize the error. \n",
    "\n",
    "$\\frac{\\partial Err}{\\partial w_2} = \\frac{\\partial Err}{\\partial y} \\frac{\\partial y}{\\partial w_2} = \\frac{\\partial Err}{\\partial y} \\frac{\\partial y}{\\partial o_1} \\frac{\\partial o_1}{\\partial w_2}, $\n",
    "\n",
    "$and, \\space \\frac{\\partial Err}{\\partial w_2} = (-y_t + y) * \\frac{\\partial \\sigma}{\\partial o_1} * \\sigma (h_1)$\n",
    "\n",
    "$and, \\space \\frac{\\partial Err}{\\partial w_2} = (-y_t + y) * \\sigma(o_1) (1-\\sigma(o_1)) * \\sigma(h_1)$\n",
    "\n",
    "The weight $w_2$ is updated, $w_2 = w_2 - \\eta * \\frac{\\partial Err}{\\partial w_2}, where \\space \\eta = the\\ learning\\ rate$\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$\\frac{\\partial Err}{\\partial w_1} = \\frac{\\partial Err}{\\partial y} \\frac{\\partial y}{\\partial w_1} = \\frac{\\partial Err}{\\partial y} \\frac{\\partial y}{\\partial o_1} \\frac{\\partial o_1}{\\partial \\sigma (h_1)} \\frac{\\partial \\sigma (h_1)}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_1} $\n",
    "\n",
    "$and, \\space \\frac{\\partial Err}{\\partial w_1} = -(y_t - y) * \\sigma (o_1) (1 - \\sigma (o_1)) * w_2 * \\sigma(h_1)(1-\\sigma(h_1))*x_1$\n",
    "\n",
    "Also, the weight $w_1$ is updated, $w_1 = w_1 - \\eta * \\frac{\\partial Err}{\\partial w_1} $\n",
    "\n",
    "Note: the error, Err, is the function of $y, \\sigma, o_i, h_i, and \\space w_i, where \\space o_i, h_i$ are output and hidden activation potentials, respectively. The basic idea is to minimize Err with respect to the weights $w_1 and \\space w_2$. Also, note that the biases $b_1, b_2$ are assuemd to be constant for simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - forward, solve y given weights and biases\n",
    "import numpy as np\n",
    "w11, w12, w21, w22, w13, w23 = 0.2, 0.15, 0.25, 0.3, 0.15, 0.1\n",
    "b1, b2, b3 = -1, -1, -1\n",
    "x1, x2 = 1, 1\n",
    "\n",
    "def sigmoid(v):\n",
    "    return 1/(1 + np.exp(-v))\n",
    "h1 = x1 * w11 + x2 * w21 + b1\n",
    "h2 = x1 * w12 + x2 * w22 + b2\n",
    "o1 = sigmoid(h1) * w13 + sigmoid(h2) * w23 + b3\n",
    "y = sigmoid(o1)\n",
    "print(\"y=\",y)\n",
    "\n",
    "# Example - backpropagation, determine the weights using a mean square error\n",
    "yt, eta = 0, 0.1\n",
    "dErrdw13 = -(yt-y)* sigmoid(o1)*(1-sigmoid(o1))*sigmoid(h1)\n",
    "w13 = w13 - eta * dErrdw13\n",
    "print(\"w13=\", w13)\n",
    "\n",
    "dErrdw23 = -(yt-y)*sigmoid(o1)*(1-sigmoid(o1))*sigmoid(h2)\n",
    "w23 = w23 - eta * dErrdw23\n",
    "print(\"w23=\", w23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fig.2`\n",
    "\n",
    "![Fig.2](ai.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try Proving it Mathmatically\n",
    "\n",
    "$\\space\\space\\space\\space\\space\\space\\space$ Given the function: $\\sigma(v) = \\frac{1}{1+e^{-v}}$, prove that: $\\frac{d\\sigma}{dv}= \\sigma(v)(1-\\sigma(v))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "v = symbols(\"v\")\n",
    "sigmoid = 1/(1+exp(-v))\n",
    "dsigmoid = diff(sigmoid, v)\n",
    "simplify(dsigmoid - sigmoid*(1-sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter - The Learning Rate\n",
    "\n",
    "Change the learning rate to see how the convergence is affected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvtest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
